{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第11回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. Attentionを用いたキャプション生成モデルを実装せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データのtrain_X, train_yのみが与えられます\n",
    "    - train_Xとtrain_yをtrain_X, valid_xとtrain_y, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - **cost関数を戻り値**としてください\n",
    "- **test_X, test_yに対する交差エントロピー(負の対数尤度)の平均で評価**します\n",
    "- **lecture_chap11_imgcap_exercise_pub.ipynb**でダウンロードしたデータを使用します\n",
    "- 全体の実行時間がiLect上で120分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください (必要なものは全てhomework関数に入れてください)\n",
    "- 解答提出時には Answer Cell の内容のみを提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルのhomework関数を完成させて提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y):\n",
    "    global vocab_size, sess, x, d # 下記Checker Cell内 で定義した各変数を利用可能.\n",
    "    \n",
    "    # WRITE ME!\n",
    "    #  Hint: 下記Checker Cell内でimportした pad_sequences, VGG16 は利用可能.\n",
    "    \n",
    "    return cost # 返り値のcostは,tensorflowの計算グラフのcostを返す."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checker Cell (for student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "rng = np.random.RandomState(42)\n",
    "random_state = 42\n",
    "\n",
    "del [\n",
    "    tf.app,\n",
    "    tf.compat,\n",
    "    tf.contrib,\n",
    "    tf.errors,\n",
    "    tf.gfile,\n",
    "    tf.graph_util,\n",
    "    tf.image,\n",
    "    tf.layers,\n",
    "    tf.logging,\n",
    "    tf.losses,\n",
    "    tf.metrics,\n",
    "    tf.python_io,\n",
    "    tf.resource_loader,\n",
    "    tf.saved_model,\n",
    "    tf.sdca,\n",
    "    tf.sets,\n",
    "    tf.summary,\n",
    "    tf.sysconfig,\n",
    "    tf.test\n",
    "]\n",
    "\n",
    "sys.modules['keras'] = None\n",
    "\n",
    "def build_vocab(file_path, target):\n",
    "    vocab = set()\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        words = line.strip().split()\n",
    "        vocab.update(words)\n",
    "\n",
    "    if target:\n",
    "        w2i = {w: np.int32(i+2) for i, w in enumerate(vocab)}\n",
    "        w2i['<s>'], w2i['</s>'] = np.int32(0), np.int32(1)\n",
    "    else:\n",
    "        w2i = {w: np.int32(i) for i, w in enumerate(vocab)}\n",
    "\n",
    "    return w2i\n",
    "\n",
    "def encode(sentence, w2i):\n",
    "    encoded_sentence = []\n",
    "    for w in sentence:\n",
    "        encoded_sentence.append(w2i[w])\n",
    "    return encoded_sentence\n",
    "\n",
    "def load_data(file_path, vocab=None, w2i=None, target=True):\n",
    "    if vocab is None and w2i is None:\n",
    "        w2i = build_vocab(file_path, target)\n",
    "    \n",
    "    data = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        s = line.strip().split()\n",
    "        if target:\n",
    "            s = ['<s>'] + s + ['</s>']\n",
    "        enc = encode(s, w2i)\n",
    "        data.append(enc)\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "    return data, w2i, i2w\n",
    "\n",
    "def validate_homework():\n",
    "    global vocab_size, sess, x, d\n",
    "    \n",
    "    train_y, w2i, i2w = load_data('./mscoco_captions_10000.txt', target=True)\n",
    "    train_X = np.load('./mscoco_images_10000.npy')\n",
    "    \n",
    "    vocab_size = len(i2w)\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:100]\n",
    "    train_y_mini = train_y[:100]\n",
    "    test_X_mini = test_X[:100]\n",
    "    test_y_mini = test_y[:100]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        d = tf.placeholder(tf.int32, [None, None], name='d')\n",
    "\n",
    "        cost = homework(train_X_mini, train_y_mini)\n",
    "        # sess, x, d はグローバル変数で定義されているので homework関数内で利用可能.\n",
    "        # 返り値のcostは,tensorflowの計算グラフのcostを返す.\n",
    "        \n",
    "        test_batch_size = 32\n",
    "        n_batches_test = -(-len(test_X_mini) // test_batch_size)\n",
    "        test_costs = []\n",
    "\n",
    "        for i in range(n_batches_test):\n",
    "            start = i * test_batch_size\n",
    "            end = start + test_batch_size if (start + test_batch_size) < len(test_X_mini) else len(test_X_mini)\n",
    "\n",
    "            test_X_mb = test_X_mini[start:end]\n",
    "            test_y_mb = np.array(pad_sequences(test_y_mini[start:end], padding='post', value=-1))\n",
    "\n",
    "            test_cost = sess.run(cost, feed_dict={x: test_X_mb, d: test_y_mb})\n",
    "\n",
    "            test_costs.append(test_cost)\n",
    "            \n",
    "    print(np.mean(test_costs))\n",
    "\n",
    "def score_homework():\n",
    "    global vocab_size, sess, x, d\n",
    "    \n",
    "    train_y, w2i, i2w = load_data('./mscoco_captions_10000.txt', target=True)\n",
    "    train_X = np.load('./mscoco_images_10000.npy')\n",
    "    \n",
    "    vocab_size = len(i2w)\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        d = tf.placeholder(tf.int32, [None, None], name='d')\n",
    "\n",
    "        cost = homework(train_X, train_y)\n",
    "        # sess, x, d はグローバル変数で定義されているので homework関数内で利用可能.\n",
    "        # 返り値のcostは,tensorflowの計算グラフのcostを返す.\n",
    "        \n",
    "        test_batch_size = 32\n",
    "        n_batches_test = -(-len(test_X) // test_batch_size)\n",
    "        test_costs = []\n",
    "\n",
    "        for i in range(n_batches_test):\n",
    "            start = i * test_batch_size\n",
    "            end = start + test_batch_size if (start + test_batch_size) < len(test_X) else len(test_X)\n",
    "\n",
    "            test_X_mb = test_X[start:end]\n",
    "            test_y_mb = np.array(pad_sequences(test_y[start:end], padding='post', value=-1))\n",
    "\n",
    "            test_cost = sess.run(cost, feed_dict={x: test_X_mb, d: test_y_mb})\n",
    "\n",
    "            test_costs.append(test_cost)\n",
    "            \n",
    "    print(np.mean(test_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_homework()\n",
    "# score_homework()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
